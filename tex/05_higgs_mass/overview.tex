\section{Analysis Overview}
\label{sec:analysis_overview}
The first step to performing a precision measurement of the Higgs boson mass (\mH) is to ``observe'' many Higgs bosons.
However, production of a Higgs boson is essentially nonexistent in everyday conditions and is still extremely rare even in the high-energy \pp collisions of the LHC.
At a center-of-mass energy of 13\TeV, the total inclusive inelastic cross section of two protons colliding is 70\mb TODO: CITE.
Comparing this to the production cross section of a Higgs boson (TODO sigma(pptoH) = 59 pb) shows that a Higgs boson is produced in approximately one out of every billion \pp collisions.  TODO CITE

To complicate matters further, the Higgs boson has a \emph{very} short mean lifetime of only $1.6 \tentotheminus{22}\snd$~\cite{pdg}.
Thus, the Higgs boson is not directly detected by CMS but is instead \emph{inferred} from its stable decay products that enter the various subdetectors.
Among all the fundamental particles so far discovered, the Higgs boson bears the second heaviest mass (approximately 125\GeV), the first belonging to the top quark (Section~\ref{sec:sm}).
This gives the scalar boson sufficient energy to decay into at least 9 different final states.
\textcolor{red}{MENTION THAT NOT ALL DECAYS MAKE ON-SHELL PARTICLES?}
Each decay occurs with a different probability---the \emph{branching fraction} or \emph{branching ratio} (\br)---whose value depends on \mH as shown in Figure~\ref{fig:higgs_br}.
\begin{figure}[!htbp]
    \begin{center}
        % Figures come from:
        % https://twiki.cern.ch/twiki/bin/view/LHCPhysics/LHCHWG?redirectedfrom=LHCPhysics.LHCHXSWG#Higgs_cross_sections_and_decay_b
		\includegraphics[width=0.48\textwidth]{figures/higgsmassmeas/higgs_BR_80to200GeV.pdf}
		\includegraphics[width=0.48\textwidth]{figures/higgsmassmeas/higgs_BR_120to130GeV.pdf}
		\caption{
            The branching ratios of various Higgs boson decays as a function of the Higgs boson mass
            over a wide range (Left) and a narrow range (Right) of values.
            }
		\label{fig:higgs_br}
	\end{center}
\end{figure} 
The question then becomes, \emph{``Which decay mode of the Higgs boson is most useful for the measurement of \mH?''}.
% Real particles enter detectors in CMS which send signals to various electronics.
% Particle Flow algorithm pieces the information together to construct objects out of each event.
% Now, instead of just a deposit of energy in the ECAL and corresponding hits in the silicon tracker, the particle is identified as a newly produced electron.
% CMS records which kinds of objects came from which events and stores the information in \emph{data sets} (TODO: ref Section future).

Owing to its large signal-to-background ratio of approximately 2 and its relatively rare four-lepton final state, the \hzzfourl decay channel is selected and is called the \emph{signal} process.
Thus, a single Higgs boson will decay via the signal process into two \PZ bosons (one on-shell and one off-shell) on average only 2.6\% of the time.
In turn, each \PZ boson decays into two opposite-sign, same flavor (OSSF) leptons (\Ztolplm, where $\ell = \Pe, \mu$) on average approximately 6.7\% of the time, giving rise to four distinct final states:
\foure, \fourmu, \twoetwomu, \twomutwoe.
The branching ratio for the overall signal process is then calculated as: % B(Z->ee)=0.033632, B(Z->mumu)=0.033662
\begin{equation*}
    \BRof{\hzzfourl} = \BRof{\htozz} \left[ \BRof{\Ztolplm} \right]^2 = 1.8\tentotheminus{3}.
\end{equation*}
Thus, a signal event is expected to be produced only once in about every \emph{trillion} \pp collisions.

The strategy is then to search the \pp collision data collected and analyzed by the CMS detector (Chapter~\ref{ch:cms_detector}) for all the detected \hzzfourl events.
The task is not so straightforward;
events in the data are categorized---not by the entire decay process---but by their final state, based on which triggers fired to collect which events.
Section~\ref{sec:datasets_simul_trig} describes the triggers used for this analysis to select events with the \fourl final state found in the corresponding data sets.
For each chosen event, the subdetectors of CMS (Chapter~\ref{ch:cms_detector}) provide a plethora of track and energy-detection information to reconstruct \emph{objects}---representations of the underlying particles within the event.
The reconstructed objects are then assembled in a fashion that checks if the logic coincides with the process of interest: \hzzfourl.  % TODO: Clean up this sentence.
For example, a pair of OSSF lepton-like objects should appear to come from a \PZ-like object---\ie having a nominal mass of approximately 91\GeV and zero net electric charge---instead of, say, appearing to come from a \PH-like object.
Two such \PZ-like objects must be formed and should appear to come from a \PH-like object.
% OSSF-dilepton objects should each appear to come from a \PZ-boson-like object (\eg having a nominal mass of approximately 91\GeV and zero net electric charge)---instead of, say, coming from a Higgs-boson-like object.
All throughout, the reconstructed event must obey physics conservation laws (energy, momentum, charge, \etc) and the associated objects may even be required to pass certain detector selection criteria (\eg $\pt^\mu > 5\GeV$).
% This process hinges on the conservation of momentum, since in the longitudinal ($z$) direction the \pp collision has initial and final.
% Specifically, the 
%     - The \PZ boson has a precisely measured mass of TODO a neutral particle, so the two leptons into which it decays should combine to Group two leptons together, 
%     - Form two different pairs of opposite-sign, same-flavor (OSSF) leptons
%     - If it appears that the to select specific hzz4l events (\emph{event selection}).
These criteria are analysis-specific and are collectively called the \emph{event selection} of the analysis.
The event selection for this analysis is described in Section~\ref{sec:evt_sel}.  % TODO: ref may be wrong.

Even though the event selection is constructed to select signal events, it is not guaranteed;
there are certain physics process that have exactly the same initial and final states as the signal process.
Such processes ``contaminate'' the collected signal events and are called \emph{background processes}.
Figure~\ref{TODO} shows how identical initial state gluons can react to produce exactly the same final state particles, while producing different intermediate particles:
the signal process (Left), initiated by gluon-gluon fusion \vs a background process (Right) which skips the intermediate Higgs boson.
It is imperative for all physics analyses to maximize the number of collected signal events while minimizing the number of collected background events.
Section~\ref{sec:bkg_estim} discusses the associated background processes and how to estimate the number of events these contribute to the signal region.
\begin{figure}[!htbp]
	\begin{center}
		\includegraphics[width=0.48\textwidth]{figures/placeholder.png}  % TODO.
		\includegraphics[width=0.48\textwidth]{figures/placeholder.png}  % TODO.
		\caption{
            Feynman diagrams showing how the initial and final states are the same
            for the signal process (\gghzzfourl, Left) and one possible background process (\ggzzfourl, Right).
        }
		\label{fig:feyndiag_sig_vs_bkg}
	\end{center}
\end{figure}

Before drawing conclusions from the data themselves, it is necessary for particle physicists to make predictions about their analysis using simulated samples or \emph{simulation}.
These samples contain simulated events usually of a specific process (\eg $\pp \to \hzzfourl$), governed by some theoretical framework that is programmed mathematically into the software package.
% events from simulated particle physics collisions--- are created by software physicists have created software packages that simulate particle physics collisions,
% the resulting particle transformations using various theoretical frameworks,
% and even the interactions that particles have with the virtual detectors, through which they traverse.
Programs like \MGvATNLO and \POWHEG can simulate millions of rare (or even \emph{fictitious}) events, which might otherwise take many years to observe in data.
Furthermore, software can even simulate the particles as they travel through the simulated detectors.
Programs like \GEANTfour can show analysts what to expect as the particles interact with a virtual version of the CMS detector.
Predictions from simulation can then be compared to the truth---the data---as a way to check the accuracy of the analysis.
For example, a surplus of events in data where none was expected may lead to the discovery of new particles, as was the case in the discovery of the Higgs boson.
% agreement or deviations in what was expected.
The simulated samples for this analysis are described in Section~\ref{sec:datasets_simul_trig}.

So how is the measurement of \mH achieved?
Since the signal process is \hzzfourl, conservation of energy leads one to expect that $\mH \approx \mfourl$.
Although this is not how the final measurement is obtained, it is a logical starting point.
The distribution of \mfourl values reveals the Higgs boson mass resonance (Figure~\ref{fig:m4l_run2}).
Simulation is then used to predict the \emph{line shape} of this signal peak (Section~\ref{sec:signal_model}).
This signal modeling is performed using a double-sided Crystal Ball function to fit the line shape, for various mass points of \mH, in each of the four final states.
% m4l dist Full Run 2.
%%%%%%%%%%%%%%%%%%%%
\begin{figure}[pbth]
    \centering
    \includegraphics[width=10cm,height=10cm,keepaspectratio]{figures/higgsmassmeas/m4l_FullRun2_epjc.jpeg}
        \caption{Distribution of \mfourl from \hzzfourl events using Full Run 2 data.}
        \label{fig:m4l_run2}
\end{figure}

In order to improve the precision of the measurement of \mH, several techniques are implemented.
The first technique is called the \Zone mass constraint, which uses the benefit of the mostly on-shell \Zone mass resonance to reevaluate the momenta of the leptons that went into building the \Zone.
This improves the lepton momentum resolution, thereby improving the resolution of the \mH peak.
The second technique is event-by-event mass uncertainty.
The third technique is vertex constraint.

% is mostly on-shell as opposed to the \Ztwo.
% Z1 significantly on-shell, but Z2 is mostly off-shell.
% 	- Therefore just perform constraint on mZ1.
% 	- Idea is to reevaluate the lepton
% - Per-event 

The resolution of the peak 
Ways to improve the 
Do likelihood fit.
$\mathcal{L}$